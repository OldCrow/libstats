# LibStats v0.9.2 Implementation Task List

**Document Version:** 2.0  
**Created:** 2025-08-16  
**Updated:** 2025-08-17  
**Target Milestone:** v0.9.2 (Proper Cache Architecture Implementation)  
**Estimated Timeline:** 2-3 weeks  
**Dependencies:** v0.9.1 completed (CACHE_AWARE strategy removed, GPU_ACCELERATED working)  

---

## üéØ Executive Summary

v0.9.2 implements the **correct caching architecture** after v0.9.1 removed the fundamentally flawed individual result caching. This release focuses on high-value parameter-level caching and mathematical function memoization, providing massive performance and memory improvements.

**Strategic Focus:** Implement proper caching architecture with massive performance improvements:
1. **Parameter-dependent computations** (90-99% memory savings, high reuse)
2. **Mathematical function results** with precision rounding (60-90% hit rates)
3. **Algorithm strategy decisions** (performance learning)
4. **Foundation for distribution refactoring** in v0.9.4

---

## üìä Current State Analysis

### ‚úÖ **Foundation from v0.9.1**
- CACHE_AWARE strategy eliminated (100x performance regression resolved)
- GPU_ACCELERATED placeholder established
- AdaptiveCache infrastructure intact but unused for individual results
- Clear path for proper caching implementation

### üéØ **Opportunity Analysis from Diagnostics**
Based on **DISTRIBUTION_CACHE_CONSOLIDATION_ANALYSIS.md**:

**Current Distribution Cache Duplication:**
- **100 Gaussian(0,1) instances:** ~10KB individual caches vs ~100 bytes centralized = **99% memory savings**
- **Every GaussianDistribution recalculates:** `normalizationConstant_ = 1/(œÉ‚àö(2œÄ))`, `negHalfSigmaSquaredInv_ = -1/(2œÉ¬≤)`
- **Parameter fitting workflows:** 95% memory reduction opportunity
- **Mathematical functions:** `gamma()`, `erf()`, `beta()` recalculated repeatedly

---

## üèóÔ∏è Implementation Plan

## **Phase 1: Centralized Parameter Cache Architecture** üíé **HIGH VALUE**
**Timeline:** 8-12 hours  
**Risk:** Medium (new architecture)  
**Dependencies:** v0.9.1 complete

### **Objective**
Replace individual distribution parameter caches with centralized system providing cross-instance sharing and massive memory reduction.

### **Task 1.1: Design Centralized Cache System**

**Create:** `include/cache/centralized_distribution_cache.h`

**Core Architecture:**
```cpp
namespace libstats {
namespace cache {

/**
 * @brief Centralized cache for distribution parameter computations
 * 
 * Replaces individual distribution instance caches with shared system
 * providing 90-99% memory savings and cross-instance parameter reuse.
 */
class CentralizedDistributionCache {
public:
    // Parameter cache templates for each distribution type
    template<typename Distribution>
    struct ParameterCacheTraits;
    
    /**
     * @brief Get cached parameters for a distribution
     * @param param_key Unique key for parameter combination (e.g., "gaussian_0.0_1.0")
     * @return Cached parameters if available
     */
    template<typename Distribution>
    std::optional<typename ParameterCacheTraits<Distribution>::CachedParams> 
    getParameters(const std::string& param_key) const;
    
    /**
     * @brief Cache computed parameters for future reuse
     * @param param_key Unique key for parameter combination
     * @param params Computed parameter values to cache
     */
    template<typename Distribution>
    void putParameters(const std::string& param_key, 
                      const typename ParameterCacheTraits<Distribution>::CachedParams& params);
    
    /**
     * @brief Get singleton instance
     */
    static CentralizedDistributionCache& instance();
    
private:
    // Separate caches for each distribution type to avoid type erasure overhead
    AdaptiveCache<std::string, GaussianParams> gaussian_cache_;
    AdaptiveCache<std::string, PoissonParams> poisson_cache_;
    AdaptiveCache<std::string, UniformParams> uniform_cache_;
    AdaptiveCache<std::string, ExponentialParams> exponential_cache_;
    AdaptiveCache<std::string, GammaParams> gamma_cache_;
    AdaptiveCache<std::string, DiscreteParams> discrete_cache_;
    
    mutable std::shared_mutex cache_mutex_;
};

// Specialization for Gaussian distribution
template<>
struct ParameterCacheTraits<GaussianDistribution> {
    struct CachedParams {
        double normalizationConstant;        // 1/(œÉ‚àö(2œÄ))
        double negHalfSigmaSquaredInv;      // -1/(2œÉ¬≤) 
        double logStandardDeviation;        // log(œÉ)
        double sigmaSqrt2;                  // œÉ‚àö2
        double invStandardDeviation;        // 1/œÉ
        double cachedSigmaSquared;          // œÉ¬≤
        double cachedTwoSigmaSquared;       // 2œÉ¬≤
        double cachedLogTwoSigmaSquared;    // log(2œÉ¬≤)
        double cachedInvSigmaSquared;       // 1/œÉ¬≤
        double cachedSqrtTwoPi;             // ‚àö(2œÄ)
        
        // Optimization flags
        bool isStandardNormal;              // Œº=0, œÉ=1 optimization
        bool isUnitVariance;                // œÉ¬≤=1 optimization  
        bool isZeroMean;                    // Œº=0 optimization
        bool isHighPrecision;               // High precision mode
        bool isLowVariance;                 // œÉ¬≤ < 0.0625 optimization
    };
    
    static std::string createKey(double mean, double std_dev) {
        return "gaussian_" + std::to_string(mean) + "_" + std::to_string(std_dev);
    }
};

// Similar specializations for other distributions...
template<>
struct ParameterCacheTraits<PoissonDistribution> {
    struct CachedParams {
        double logLambda;                   // log(Œª)
        double expNegLambda;                // e^(-Œª)
        double sqrtLambda;                  // ‚àöŒª
        double logGammaLambdaPlus1;         // log(Œì(Œª+1))
        double invLambda;                   // 1/Œª
        
        // Optimization flags
        bool isSmallLambda;                 // Œª < 10 algorithm choice
        bool isLargeLambda;                 // Œª > 100 algorithm choice
        bool isVeryLargeLambda;             // Œª > 1000 algorithm choice
        bool isIntegerLambda;               // Œª ‚àà ‚Ñ§ optimization
        bool isTinyLambda;                  // Œª < 0.1 series expansion
    };
    
    static std::string createKey(double lambda) {
        return "poisson_" + std::to_string(lambda);
    }
};

} // namespace cache
} // namespace libstats
```

**Checklist:**
- [ ] Create centralized cache header with template architecture
- [ ] Define parameter cache traits for all 6 distributions
- [ ] Implement singleton pattern with thread safety
- [ ] Add cache key generation strategies for each distribution type
- [ ] Design cache eviction policies for parameter-level data

### **Task 1.2: Migrate Gaussian Distribution to Centralized Cache**

**Files to Modify:**
- `include/distributions/gaussian.h` - Remove individual cache members
- `src/gaussian.cpp` - Use centralized cache in `updateCacheUnsafe()`

**Implementation Pattern:**
```cpp
// BEFORE (in gaussian.h) - Remove these individual cache members
class GaussianDistribution {
private:
    mutable std::shared_mutex cache_mutex_;           // 16 bytes per instance
    mutable bool cache_valid_;                        // 1 byte per instance  
    mutable double normalizationConstant_;            // 8 bytes per instance
    mutable double negHalfSigmaSquaredInv_;           // 8 bytes per instance
    // ... 10+ cached values = ~100 bytes per instance
};

// AFTER (in gaussian.h) - Minimal cache footprint
class GaussianDistribution {
private:
    mutable std::optional<std::string> parameter_cache_key_;  // ~32 bytes total
    mutable bool cache_needs_update_{true};
    
    const auto& getCachedParameters() const {
        using CacheTraits = cache::ParameterCacheTraits<GaussianDistribution>;
        
        if (!parameter_cache_key_.has_value() || cache_needs_update_) {
            parameter_cache_key_ = CacheTraits::createKey(mean_.load(), standardDeviation_.load());
            cache_needs_update_ = false;
        }
        
        auto& central_cache = cache::CentralizedDistributionCache::instance();
        auto cached = central_cache.getParameters<GaussianDistribution>(*parameter_cache_key_);
        
        if (!cached.has_value()) {
            // Compute parameters once, cache centrally
            CacheTraits::CachedParams params;
            computeParameters(params);  // Fill in all the expensive calculations
            central_cache.putParameters<GaussianDistribution>(*parameter_cache_key_, params);
            return central_cache.getParameters<GaussianDistribution>(*parameter_cache_key_).value();
        }
        
        return cached.value();
    }
    
    void computeParameters(cache::ParameterCacheTraits<GaussianDistribution>::CachedParams& params) const {
        double sigma = standardDeviation_.load();
        double mu = mean_.load();
        
        // All the expensive math happens once and gets cached
        params.normalizationConstant = 1.0 / (sigma * constants::math::SQRT_2PI);
        params.negHalfSigmaSquaredInv = -1.0 / (2.0 * sigma * sigma);
        params.logStandardDeviation = std::log(sigma);
        // ... compute all other expensive parameters
        
        // Set optimization flags
        params.isStandardNormal = (std::abs(mu) < 1e-15 && std::abs(sigma - 1.0) < 1e-15);
        params.isUnitVariance = (std::abs(sigma - 1.0) < 1e-15);
        params.isZeroMean = (std::abs(mu) < 1e-15);
        // ... set other flags
    }
    
    void invalidateCache() const noexcept {
        cache_needs_update_ = true;
    }
};
```

**Checklist:**
- [ ] Remove individual cache members from GaussianDistribution
- [ ] Implement `getCachedParameters()` using centralized cache
- [ ] Update all methods to use centralized cached parameters
- [ ] Ensure `invalidateCache()` works with centralized system
- [ ] Verify thread safety of parameter access

### **Task 1.3: Migrate Remaining Distributions**

**Priority Order (by complexity and usage):**
1. **PoissonDistribution** - Complex mathematical functions, high impact
2. **UniformDistribution** - Simple but frequently used
3. **ExponentialDistribution** - Moderate complexity
4. **GammaDistribution** - Complex gamma functions
5. **DiscreteDistribution** - Simple integer operations

**Pattern for Each Distribution:**
- Remove individual cache members (~100 bytes per instance)
- Define `ParameterCacheTraits` specialization
- Implement centralized cache integration
- Update `updateCacheUnsafe()` to use centralized system
- Verify all cached parameters are properly migrated

**Checklist:**
- [ ] Migrate PoissonDistribution to centralized cache
- [ ] Migrate UniformDistribution to centralized cache  
- [ ] Migrate ExponentialDistribution to centralized cache
- [ ] Migrate GammaDistribution to centralized cache
- [ ] Migrate DiscreteDistribution to centralized cache
- [ ] Update all factory methods to work with centralized caching

---

## **Phase 2: Mathematical Function Caching** üßÆ **HIGH VALUE**
**Timeline:** 6-8 hours  
**Risk:** Low (builds on proven AdaptiveCache)  
**Dependencies:** Phase 1 in progress

### **Objective**
Implement mathematical function memoization with precision rounding to achieve 60-90% cache hit rates for expensive mathematical operations.

### **Task 2.1: Mathematical Function Cache Infrastructure**

**Create:** `include/cache/math_function_cache.h`

**Core Implementation:**
```cpp
namespace libstats {
namespace cache {

/**
 * @brief High-performance mathematical function cache with precision rounding
 * 
 * Caches expensive mathematical functions used across distributions with
 * configurable precision to balance accuracy vs cache hit rates.
 */
class MathFunctionCache {
public:
    /**
     * @brief Get cached gamma function result
     * @param x Input value
     * @param precision Rounding precision (default: 0.001)
     * @return Cached or computed gamma function result
     */
    static double getCachedGamma(double x, double precision = 0.001) {
        double rounded = std::round(x / precision) * precision;
        std::string key = "gamma_" + std::to_string(rounded) + "_" + std::to_string(precision);
        
        auto& cache = getGammaCache();
        auto cached = cache.get(key);
        if (cached.has_value()) {
            return *cached;
        }
        
        // Compute and cache
        double result = math::gamma_function(x);  // Expensive computation
        cache.put(key, result);
        return result;
    }
    
    /**
     * @brief Get cached error function result  
     * @param x Input value
     * @param precision Rounding precision (default: 0.0001)
     * @return Cached or computed erf(x)
     */
    static double getCachedErf(double x, double precision = 0.0001) {
        double rounded = std::round(x / precision) * precision;
        std::string key = "erf_" + std::to_string(rounded) + "_" + std::to_string(precision);
        
        auto& cache = getErfCache();
        auto cached = cache.get(key);
        if (cached.has_value()) {
            return *cached;
        }
        
        double result = math::erf(x);  // Expensive computation
        cache.put(key, result);
        return result;
    }
    
    /**
     * @brief Get cached complementary error function result
     */
    static double getCachedErfc(double x, double precision = 0.0001);
    
    /**
     * @brief Get cached beta function result
     */
    static double getCachedBeta(double a, double b, double precision = 0.001);
    
    /**
     * @brief Get cached natural logarithm result
     */
    static double getCachedLog(double x, double precision = 0.0001);
    
    /**
     * @brief Clear all mathematical function caches
     */
    static void clearAll();
    
    /**
     * @brief Get cache statistics
     */
    struct CacheStats {
        size_t gamma_cache_size;
        size_t erf_cache_size;
        size_t beta_cache_size;
        size_t log_cache_size;
        double gamma_hit_rate;
        double erf_hit_rate;
        double beta_hit_rate;
        double log_hit_rate;
    };
    static CacheStats getStats();
    
private:
    static AdaptiveCache<std::string, double>& getGammaCache();
    static AdaptiveCache<std::string, double>& getErfCache();
    static AdaptiveCache<std::string, double>& getErfcCache();
    static AdaptiveCache<std::string, double>& getBetaCache();
    static AdaptiveCache<std::string, double>& getLogCache();
};

} // namespace cache
} // namespace libstats
```

**Checklist:**
- [ ] Implement mathematical function cache infrastructure
- [ ] Add precision rounding logic for cache key generation
- [ ] Create separate caches for different function types
- [ ] Implement cache statistics and monitoring
- [ ] Add cache clearing and management utilities

### **Task 2.2: Integrate Math Function Caching into Distributions**

**Target Functions by Distribution:**

**PoissonDistribution:**
```cpp
// BEFORE - Direct computation
double logGammaLambdaPlus1 = math::lgamma(lambda_ + 1.0);

// AFTER - Cached computation
double logGammaLambdaPlus1 = cache::MathFunctionCache::getCachedGamma(lambda_ + 1.0);
```

**GaussianDistribution:**
```cpp
// BEFORE - Direct computation in CDF
double erf_result = math::erf(normalized_x);

// AFTER - Cached computation  
double erf_result = cache::MathFunctionCache::getCachedErf(normalized_x, 0.0001);
```

**GammaDistribution:**
```cpp
// BEFORE - Direct computation
double gamma_alpha = math::gamma_function(alpha_);
double beta_result = math::beta_function(alpha_, beta_);

// AFTER - Cached computation
double gamma_alpha = cache::MathFunctionCache::getCachedGamma(alpha_, 0.001);
double beta_result = cache::MathFunctionCache::getCachedBeta(alpha_, beta_, 0.001);
```

**Implementation Strategy:**
- Replace direct math function calls with cached versions
- Use appropriate precision for each function type
- Monitor cache hit rates during development
- Adjust precision based on hit rate vs accuracy trade-offs

**Checklist:**
- [ ] Replace gamma function calls in PoissonDistribution
- [ ] Replace erf/erfc calls in GaussianDistribution CDF methods
- [ ] Replace beta function calls in GammaDistribution  
- [ ] Replace logarithm calls where beneficial
- [ ] Monitor cache hit rates and adjust precision settings
- [ ] Verify numerical accuracy is maintained

---

## **Phase 3: Performance Strategy Caching** üìä **MEDIUM VALUE**
**Timeline:** 4-6 hours  
**Risk:** Low (leverages existing performance history)  
**Dependencies:** Phase 1 complete

### **Objective**
Cache algorithm strategy decisions and performance learning to optimize batch operation dispatch.

### **Task 3.1: Algorithm Strategy Cache**

**Create:** `include/cache/strategy_cache.h`

**Implementation:**
```cpp
namespace libstats {
namespace cache {

/**
 * @brief Cache for optimal algorithm strategies and performance metadata
 */
class StrategyCache {
public:
    struct OptimalStrategy {
        performance::Strategy strategy;
        size_t optimal_grain_size;
        double expected_speedup;
        std::chrono::steady_clock::time_point last_updated;
    };
    
    /**
     * @brief Get cached optimal strategy for given conditions
     * @param dist_type Distribution type
     * @param operation Operation name (pdf, cdf, etc.)
     * @param batch_size Size of batch operation
     * @return Cached strategy if available
     */
    std::optional<OptimalStrategy> getStrategy(
        performance::DistributionType dist_type,
        const std::string& operation,
        size_t batch_size
    ) const;
    
    /**
     * @brief Cache successful strategy for future use
     */
    void putStrategy(
        performance::DistributionType dist_type,
        const std::string& operation, 
        size_t batch_size,
        const OptimalStrategy& strategy
    );
    
    /**
     * @brief Record performance for strategy learning
     */
    void recordPerformance(
        performance::DistributionType dist_type,
        const std::string& operation,
        size_t batch_size,
        performance::Strategy strategy,
        std::chrono::nanoseconds execution_time
    );
    
private:
    AdaptiveCache<std::string, OptimalStrategy> strategy_cache_;
    std::string createKey(performance::DistributionType dist_type, 
                         const std::string& operation, 
                         size_t batch_size) const;
};

} // namespace cache
} // namespace libstats
```

**Checklist:**
- [ ] Implement strategy cache infrastructure
- [ ] Integrate with existing PerformanceHistory system
- [ ] Add batch size categorization for strategy caching
- [ ] Create cache key strategies for algorithm decisions
- [ ] Add performance learning and adaptation logic

---

## **Phase 4: Memory Optimization and Consolidation** üíæ **HIGH IMPACT**
**Timeline:** 6-8 hours  
**Risk:** Medium (affects all distributions)  
**Dependencies:** Phases 1-2 complete

### **Objective** 
Measure and optimize the memory impact of centralized caching, achieving the projected 90-99% memory savings.

### **Task 4.1: Memory Usage Analysis**

**Create memory benchmarking tools:**
```cpp
// Memory usage comparison utility
class CacheMemoryAnalyzer {
public:
    struct MemoryReport {
        size_t individual_cache_memory;     // Old approach
        size_t centralized_cache_memory;    // New approach
        double memory_savings_ratio;
        size_t instance_count;
        std::string distribution_type;
    };
    
    static MemoryReport analyzeDistribution(const std::string& dist_type, size_t instance_count);
    static void printMemoryComparison();
};
```

**Benchmarking Scenarios:**
- 100 Gaussian(0,1) instances
- 1000 mixed parameter instances
- Parameter fitting workflows with frequent parameter changes

**Target Metrics:**
- **Individual cache approach:** ~100 bytes √ó N instances
- **Centralized cache approach:** ~100 bytes total (shared)
- **Expected savings:** 90-99% for common parameter patterns

**Checklist:**
- [ ] Create memory analysis utilities
- [ ] Benchmark memory usage before/after centralized caching
- [ ] Verify 90-99% memory savings in common scenarios
- [ ] Profile memory allocation patterns
- [ ] Optimize cache eviction policies for memory efficiency

### **Task 4.2: Performance Validation**

**Create performance benchmarking:**
```cpp
// Performance comparison for cached vs uncached operations
class CachePerformanceBenchmark {
public:
    struct PerformanceReport {
        std::chrono::nanoseconds cold_start_time;    // First access
        std::chrono::nanoseconds warm_cache_time;    // Cached access
        double cache_speedup_ratio;
        double cache_hit_rate;
        std::string operation_type;
    };
    
    static PerformanceReport benchmarkParameterCaching();
    static PerformanceReport benchmarkMathFunctionCaching(); 
    static void runComprehensiveBenchmark();
};
```

**Target Performance Improvements:**
- **Parameter cache hit operations:** 10-100x speedup
- **Math function cache hits:** 5-50x speedup  
- **Overall cache hit rate:** >80% for parameter operations, >60% for math functions

**Checklist:**
- [ ] Implement performance benchmarking utilities
- [ ] Measure cache hit rates across different usage patterns
- [ ] Verify performance improvements meet target ranges
- [ ] Profile cache access patterns and optimize hot paths
- [ ] Create automated performance regression tests

---

## **Phase 5: Error Handling Standardization** ‚ö†Ô∏è **MEDIUM VALUE**
**Timeline:** 4-6 hours  
**Risk:** Low (code cleanup and consistency)  
**Dependencies:** Core platform-independent improvements analysis

### **Objective**
Standardize error handling patterns across all distributions and core modules to improve debugging, user experience, and code maintainability.

### **Task 5.1: Standardize Distribution Error Patterns**

**Files to Update:**
- All distribution headers and implementations (gaussian.h/cpp, poisson.h/cpp, etc.)
- `include/core/error_handling.h`

**Current Issues from Analysis:**
- Inconsistent error return patterns between distributions
- Mix of exceptions, std::expected, and invalid state handling
- Error messages vary in format and informativeness
- Some edge cases return invalid results instead of proper errors

**Implementation Pattern:**
```cpp
// Standardize to consistent error handling across all distributions
namespace libstats {
namespace core {

/**
 * @brief Standardized error categories for libstats operations
 */
enum class ErrorCategory {
    InvalidParameter,
    NumericalInstability, 
    ComputationOverflow,
    CacheError,
    ConfigurationError
};

/**
 * @brief Detailed error information with context
 */
struct ErrorInfo {
    ErrorCategory category;
    std::string message;
    std::string context;  // Function/operation where error occurred
    std::optional<double> problematic_value;
    std::string suggested_action;
};

/**
 * @brief Standard result type for all distribution operations
 */
template<typename T>
using Result = std::expected<T, ErrorInfo>;

/**
 * @brief Helper for creating standardized error results
 */
template<typename T>
Result<T> makeError(ErrorCategory category, const std::string& message, 
                   const std::string& context = "",
                   std::optional<double> value = std::nullopt,
                   const std::string& suggestion = "") {
    return std::unexpected(ErrorInfo{
        .category = category,
        .message = message,
        .context = context,
        .problematic_value = value,
        .suggested_action = suggestion
    });
}

} // namespace core
} // namespace libstats
```

**Checklist:**
- [ ] Define standardized error categories and ErrorInfo structure
- [ ] Update GaussianDistribution parameter validation with standard errors
- [ ] Update PoissonDistribution lambda validation (Œª > 0) with standard errors  
- [ ] Update UniformDistribution range validation (min < max) with standard errors
- [ ] Update ExponentialDistribution rate validation (rate > 0) with standard errors
- [ ] Update GammaDistribution shape/scale validation with standard errors
- [ ] Update DiscreteDistribution probability validation with standard errors

### **Task 5.2: Numerical Edge Case Handling**

**Target Issues:**
- PDF/CDF functions returning NaN or Inf without proper error indication
- Underflow/overflow in mathematical computations
- Parameter values at domain boundaries (e.g., œÉ = 0, Œª = 0)

**Implementation Strategy:**
```cpp
// Example for GaussianDistribution PDF with standardized error handling
core::Result<double> GaussianDistribution::pdf(double x) const {
    double sigma = standardDeviation_.load();
    double mu = mean_.load();
    
    // Validate parameters
    if (sigma <= 0.0) {
        return core::makeError<double>(
            core::ErrorCategory::InvalidParameter,
            "Standard deviation must be positive",
            "GaussianDistribution::pdf",
            sigma,
            "Use setStandardDeviation() with a positive value"
        );
    }
    
    // Check for numerical edge cases
    double z = (x - mu) / sigma;
    if (std::abs(z) > 38.0) {  // Prevents underflow in exp(-0.5*z¬≤)
        return (z > 0) ? 0.0 : 0.0;  // Return 0 for extreme values
    }
    
    const auto& params = getCachedParameters();
    double result = params.normalizationConstant * std::exp(params.negHalfSigmaSquaredInv * z * z);
    
    // Validate result
    if (!std::isfinite(result)) {
        return core::makeError<double>(
            core::ErrorCategory::NumericalInstability,
            "PDF computation resulted in non-finite value", 
            "GaussianDistribution::pdf",
            x,
            "Check input value is within reasonable bounds"
        );
    }
    
    return result;
}
```

**Checklist:**
- [ ] Add numerical stability checks to all PDF implementations
- [ ] Add overflow/underflow protection to all CDF implementations
- [ ] Standardize parameter boundary condition handling
- [ ] Add domain validation for quantile functions
- [ ] Implement consistent NaN/Inf detection and error reporting

### **Task 5.3: Error Message Standardization**

**Create:** `include/core/error_messages.h`

**Implementation:**
```cpp
namespace libstats {
namespace core {
namespace error_messages {

/**
 * @brief Standardized error message templates
 */
struct Messages {
    // Parameter validation messages
    static constexpr const char* POSITIVE_PARAMETER = "{} must be positive, got {}";
    static constexpr const char* FINITE_PARAMETER = "{} must be finite, got {}";
    static constexpr const char* RANGE_PARAMETER = "{} must be in range [{}, {}], got {}";
    static constexpr const char* NON_EMPTY_PARAMETER = "{} cannot be empty";
    
    // Computation error messages
    static constexpr const char* NUMERICAL_OVERFLOW = "Computation overflow in {} with input {}";
    static constexpr const char* NUMERICAL_UNDERFLOW = "Computation underflow in {} with input {}";
    static constexpr const char* NON_FINITE_RESULT = "Non-finite result in {} with input {}";
    
    // Cache error messages
    static constexpr const char* CACHE_CORRUPTION = "Cache corruption detected in {}";
    static constexpr const char* CACHE_EVICTION_FAILED = "Failed to evict cache entry in {}";
    
    // Suggestions
    static constexpr const char* CHECK_INPUT_BOUNDS = "Verify input values are within expected domain";
    static constexpr const char* INCREASE_PRECISION = "Consider using higher precision arithmetic";
    static constexpr const char* CONTACT_SUPPORT = "Contact library maintainers if issue persists";
};

/**
 * @brief Helper functions for formatted error messages
 */
std::string formatParameterError(const std::string& param_name, double value, const std::string& constraint);
std::string formatNumericalError(const std::string& operation, double input_value, const std::string& issue_type);
std::string formatCacheError(const std::string& cache_type, const std::string& operation);

} // namespace error_messages
} // namespace core
} // namespace libstats
```

**Checklist:**
- [ ] Create standardized error message templates
- [ ] Implement formatted error message helpers
- [ ] Update all distribution classes to use standard error messages
- [ ] Ensure error messages include actionable suggestions
- [ ] Add context information (function names, parameter values) to all errors

### **Task 5.4: Error Handling Integration Tests**

**Create comprehensive error handling test suite:**
```cpp
// Error handling validation tests
TEST(ErrorHandlingStandardizationTest, InvalidParameterErrors) {
    // Test all distributions respond to invalid parameters with standard errors
    auto gaussian_result = GaussianDistribution::create(0.0, -1.0);  // Invalid œÉ
    ASSERT_FALSE(gaussian_result.has_value());
    ASSERT_EQ(gaussian_result.error().category, core::ErrorCategory::InvalidParameter);
    ASSERT_FALSE(gaussian_result.error().suggested_action.empty());
}

TEST(ErrorHandlingStandardizationTest, NumericalEdgeCases) {
    // Test numerical edge cases return proper errors rather than NaN/Inf
    auto gaussian = GaussianDistribution::create(0.0, 1.0).value();
    auto pdf_result = gaussian.pdf(1e10);  // Extreme value
    
    ASSERT_TRUE(pdf_result.has_value());  // Should handle gracefully
    ASSERT_TRUE(std::isfinite(pdf_result.value()));
}

TEST(ErrorHandlingStandardizationTest, ErrorMessageConsistency) {
    // Test error messages follow standard format across distributions
    // All parameter validation errors should include parameter name and suggestion
}
```

**Checklist:**
- [ ] Create comprehensive error handling test suite
- [ ] Test all invalid parameter combinations across distributions
- [ ] Test numerical edge cases and overflow conditions
- [ ] Verify error message consistency and informativeness
- [ ] Test error handling performance (errors shouldn't be expensive to generate)

**Success Criteria:**
- [ ] **Consistency:** All distributions use identical error handling patterns
- [ ] **Informativeness:** Error messages include context, values, and suggestions  
- [ ] **Reliability:** No NaN/Inf returns without proper error indication
- [ ] **Performance:** Error handling doesn't impact normal operation performance
- [ ] **Debuggability:** Clear error categories enable easy debugging

---

## üß™ **Testing and Validation Strategy**

### **Unit Tests**
```cpp
// Parameter caching tests
TEST(CentralizedCacheTest, GaussianParameterCaching) {
    // Verify multiple instances share cached parameters
    auto dist1 = GaussianDistribution::create(0.0, 1.0).value;
    auto dist2 = GaussianDistribution::create(0.0, 1.0).value;
    
    // Both should use same cached parameters (verify memory sharing)
}

// Math function caching tests
TEST(MathFunctionCacheTest, GammaCacheHitRate) {
    // Test precision rounding leads to cache hits
    double result1 = MathFunctionCache::getCachedGamma(2.0001, 0.001);
    double result2 = MathFunctionCache::getCachedGamma(2.0002, 0.001);
    
    // Should return same cached result due to precision rounding
}

// Memory usage tests
TEST(CacheMemoryTest, MemorySavingsValidation) {
    // Measure memory before/after centralized caching
    // Verify 90%+ savings for common scenarios
}
```

### **Integration Tests**
```cpp
// End-to-end caching workflow
TEST(CacheIntegrationTest, ParameterFittingWorkflow) {
    // Simulate parameter fitting with repeated parameter combinations
    // Verify cache hit rates and performance improvements
}

// Cross-distribution interaction
TEST(CacheIntegrationTest, MultiDistributionCaching) {
    // Test multiple distribution types using centralized cache
    // Verify no cache key collisions or interference
}
```

### **Performance Tests**
```bash
# Automated performance validation
./benchmarks/cache_performance_suite
# Should show:
# - Parameter cache hit rate >80%
# - Math function cache hit rate >60% 
# - Memory usage reduction 90-99%
# - No performance regression in uncached operations
```

---

## üéØ **Success Criteria**

### **Phase 1: Centralized Parameter Cache**
- [ ] **Memory reduction:** 90-99% savings for common parameter patterns
- [ ] **Performance improvement:** 10-100x speedup for parameter cache hits
- [ ] **Cross-instance sharing:** Multiple distribution instances use shared cached parameters
- [ ] **Thread safety:** All concurrent access patterns work correctly
- [ ] **Cache hit rate:** >80% for parameter-heavy operations

### **Phase 2: Mathematical Function Cache**  
- [ ] **Cache hit rate:** >60% for mathematical function calls
- [ ] **Performance improvement:** 5-50x speedup for cached function calls
- [ ] **Numerical accuracy:** No degradation in computational precision
- [ ] **Memory efficiency:** Reasonable memory usage vs hit rate trade-off
- [ ] **Function coverage:** All major expensive math functions cached

### **Phase 3: Strategy Cache**
- [ ] **Algorithm optimization:** Consistently selects optimal strategies for given conditions
- [ ] **Learning effectiveness:** Performance improves over time with usage
- [ ] **Cache coherence:** Strategy cache works with existing PerformanceHistory system
- [ ] **Batch size adaptation:** Optimal strategies cached for different scales

### **Phase 4: Memory Optimization**
- [ ] **Memory analysis:** Comprehensive before/after memory usage documentation
- [ ] **Performance validation:** No regression in non-cached operations
- [ ] **Scalability:** Cache performance scales well with instance count
- [ ] **Resource management:** Effective cache eviction prevents memory exhaustion

---

## üö® **Risk Assessment and Mitigation**

### **Technical Risks**

**Risk:** Thread safety issues in centralized cache access  
**Mitigation:** Comprehensive lock-free design where possible, thorough concurrent testing

**Risk:** Cache key collisions between different distributions  
**Mitigation:** Strong type safety in cache key generation, namespace separation

**Risk:** Memory explosion from aggressive caching  
**Mitigation:** Adaptive eviction policies, memory monitoring, configurable limits

**Risk:** Reduced numerical accuracy from precision rounding  
**Mitigation:** Configurable precision levels, accuracy validation tests

### **Performance Risks**

**Risk:** Cache overhead exceeding computation savings for some operations  
**Mitigation:** Selective caching based on operation cost, performance monitoring

**Risk:** Cold cache performance worse than no caching  
**Mitigation:** Smart cache warming, background pre-computation for common patterns  

**Risk:** Cache contention under high concurrency  
**Mitigation:** Fine-grained locking, lock-free data structures, performance profiling

### **Migration Risks**

**Risk:** Breaking changes during distribution refactoring  
**Mitigation:** Incremental migration, comprehensive test coverage, rollback capability

**Risk:** Increased complexity making debugging difficult  
**Mitigation:** Extensive logging, cache introspection tools, clear documentation

---

## üìä **Expected Performance Outcomes**

### **Immediate Benefits (Phase 1-2 Complete)**
- **Memory usage:** 90-99% reduction for parameter caching scenarios
- **Parameter operations:** 10-100x speedup for cache hits
- **Math functions:** 5-50x speedup for gamma, erf, beta function calls
- **Cross-instance efficiency:** Shared computation across distribution instances

### **Long-term Benefits (All Phases Complete)**  
- **Algorithm optimization:** Consistently optimal strategy selection
- **Learning improvement:** Performance gets better with usage
- **Memory efficiency:** Sustainable memory usage even with large-scale operations
- **Foundation for GPU acceleration:** Proper caching architecture ready for v1.1.0+ GPU integration

### **Architecture Benefits**
- **Code consolidation:** Single cache implementation instead of N distribution caches  
- **Maintainability:** Clear separation between computation and caching concerns
- **Extensibility:** Easy to add new distributions with automatic caching benefits
- **Observability:** Centralized cache monitoring and diagnostics

---

**Document Status:** First Cut - Ready for Review and Expansion  
**Implementation Priority:** Phase 1 ‚Üí Phase 2 ‚Üí Phase 4 ‚Üí Phase 3  
**Next Update:** Add distribution code refactoring tasks from original roadmap  
**Dependencies:** v0.9.1 completed, CACHE_AWARE strategy removed
